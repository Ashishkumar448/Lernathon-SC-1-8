# -*- coding: utf-8 -*-
"""Learn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I39KCvl5JpdKWxrsqWy96BtgBm8xunra
"""

!pip install pandas scikit-learn xgboost matplotlib seaborn streamlit shap --quiet

import pandas as pd
import sklearn as sk
import numpy as np
import xgboost as xgb
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st
import shap
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/sample_data/food_adulteration_data.csv')
df.head()

df.info()
print(df.isnull().sum())
print(df.nunique())

sns.countplot(data=df, x='severity')
plt.title('Severity Class Distribution')
plt.show()

sns.countplot(data=df, x='health_risk')
plt.title('Health Risk Label Distribution')
plt.show()

# Binary target
df['is_adulterated'] = 1  # All rows contain an adulterant in this dataset

# Date to datetime and derive features
df['detection_date'] = pd.to_datetime(df['detection_date'])
df['month'] = df['detection_date'].dt.month
df['year'] = df['detection_date'].dt.year

# for dropping the unnecessary data which can't be used for training
drop_cols = ['adulteration_id', 'detection_method', 'action_taken', 'detection_date']
df = df.drop(columns=drop_cols)

target = 'severity'  # Or 'health_risk' or your chosen label
X = df.drop(columns=[target, 'is_adulterated'])  # Drop all target variables you are NOT using
y = df[target]

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

cat_cols = X.select_dtypes(include='object').columns
num_cols = X.select_dtypes(include='number').columns

preproc = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
], remainder='passthrough')

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

# First, label-encode your target before this loop:
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(y_train)
y_train_enc = le.transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

for name, clf in models.items():
    pipe = Pipeline([('pre', preproc), ('model', clf)])
    if name == 'XGBoost':
        pipe.fit(X_train, y_train_enc)  # Use encoded target for XGBoost
        preds = pipe.predict(X_val)
        score = f1_score(y_val_enc, preds, average='macro')
    else:
        pipe.fit(X_train, y_train)      # Use string target for RandomForest
        preds = pipe.predict(X_val)
        score = f1_score(y_val, preds, average='macro')
    print(f"{name}: F1 Score = {score:.3f}")

# Train model
from sklearn.model_selection import GridSearchCV

param_grid_rf = {
    'model__n_estimators': [200, 400],
    'model__max_depth': [None, 10, 20],
}
rf_pipe = Pipeline([('pre', preproc), ('model', RandomForestClassifier(class_weight='balanced', random_state=42))])
gs_rf = GridSearchCV(rf_pipe, param_grid_rf, cv=3, scoring='f1_macro')
gs_rf.fit(X_train, y_train)
print('Best RF F1:', gs_rf.best_score_)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
y_pred = gs_rf.best_estimator_.predict(X_test)
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)
print(classification_report(y_test, y_pred))

# test code
# XGBoost on test set:
xgb_pipe = Pipeline([('pre', preproc), ('model', models['XGBoost'])])
xgb_pipe.fit(X_train, y_train_enc)
y_pred = xgb_pipe.predict(X_test)
# Use y_test_enc and y_pred for evaluations
print(classification_report(y_test_enc, y_pred, target_names=le.classes_))
named_preds = le.inverse_transform(y_pred)
print(pd.Series(named_preds).value_counts())

# better model training
from sklearn.preprocessing import LabelEncoder

# Encode severity classes as integers
le = LabelEncoder()
le.fit(y_train)
y_train_enc = le.transform(y_train)
y_val_enc   = le.transform(y_val)
y_test_enc  = le.transform(y_test)

for name, clf in models.items():
    pipe = Pipeline([('pre', preproc), ('model', clf)])
    XGBClassifier(tree_method='hist', eval_metric='mlogloss', random_state=42)
    if name == 'XGBoost':
        pipe.fit(X_train, y_train_enc)  # Use numbers for XGBoost
        preds = pipe.predict(X_val)
        score = f1_score(y_val_enc, preds, average='macro')
        print(f"{name}: F1 Score = {score:.3f}")
    else:
        pipe.fit(X_train, y_train)      # Strings are OK for RF
        preds = pipe.predict(X_val)
        score = f1_score(y_val, preds, average='macro')
        print(f"{name}: F1 Score = {score:.3f}")

# If using XGBoost, remember to use encoded targets for test score:
best_pipe = Pipeline([('pre', preproc), ('model', models['Random Forest'])])
best_pipe.fit(X_train, y_train)
y_pred = best_pipe.predict(X_test)
print(classification_report(y_test, y_pred))  # string labels for Random Forest

# For XGBoost:
xgb_pipe = Pipeline([('pre', preproc), ('model', models['XGBoost'])])
xgb_pipe.fit(X_train, y_train_enc)
y_pred_xgb = xgb_pipe.predict(X_test)
y_pred_xgb_labels = le.inverse_transform(y_pred_xgb) # get string classes for reporting
print(classification_report(y_test, y_pred_xgb_labels))

# temporary block
if name == 'XGBoost':
    y_pred_test = pipe.predict(X_test)
    y_pred_test_labels = le.inverse_transform(y_pred_test)
    print(classification_report(y_test, y_pred_test_labels))

# model save
joblib.dump(best_pipe, 'rf_pipe.pkl')
joblib.dump(xgb_pipe, 'xgb_pipe.pkl')
joblib.dump(le, 'label_encoder.pkl') # for XGBoost